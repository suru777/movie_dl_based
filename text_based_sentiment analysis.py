# -*- coding: utf-8 -*-
"""text_based.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EugHFs1hNjvOdW59ivk_iIIt97cckPyA
"""

import re
import nltk
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf

# Use tf.keras components
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D

# Download required NLTK data packages
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

train_path = "training.csv"
val_path   = "validation.csv"
test_path  = "test.csv"

df_train = pd.read_csv('/training.csv')
df_val   = pd.read_csv('/validation.csv')
df_test  = pd.read_csv('/test.csv')

print("Training Data (head):")
print(df_train.head())
print("\nValidation Data (head):")
print(df_val.head())
print("\nTest Data (head):")
print(df_test.head())

def normalize(text):
    # Remove newline characters and extra spaces
    text = text.replace("\n", " ").replace("\r", " ")
    text = re.sub(r'\s+', ' ', text)
    # Remove punctuation and special characters
    text = re.sub(r'[^\w\s]', '', text)
    # Remove URLs
    text = re.sub(r'http\S+', '', text)
    # Trim leading/trailing whitespace
    text = text.strip()
    # Remove non-alphabetic characters (optional)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Convert text to lowercase
    text = text.lower()
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    text = ' '.join(word for word in word_tokens if word not in stop_words)
    return text

# Apply normalization to all datasets
df_train['text'] = df_train['text'].apply(normalize)
df_val['text']   = df_val['text'].apply(normalize)
df_test['text']  = df_test['text'].apply(normalize)

# Combine training and validation texts for fitting the tokenizer
combined_texts = pd.concat([df_train['text'], df_val['text']], axis=0).values
tokenizer = Tokenizer()
tokenizer.fit_on_texts(combined_texts)

# Convert texts to sequences
train_sequences = tokenizer.texts_to_sequences(df_train['text'])
val_sequences   = tokenizer.texts_to_sequences(df_val['text'])
test_sequences  = tokenizer.texts_to_sequences(df_test['text'])

# Determine the maximum sequence length based on the training set
max_length = max(len(seq) for seq in train_sequences)
print("Max sequence length (train):", max_length)

# Pad sequences to ensure uniform input length
X_train = pad_sequences(train_sequences, maxlen=max_length)
X_val   = pad_sequences(val_sequences,   maxlen=max_length)
X_test  = pad_sequences(test_sequences,    maxlen=max_length)

# Vocabulary size
word_index = tokenizer.word_index
num_words  = len(word_index) + 1
print("Vocabulary size:", num_words)

y_train = pd.get_dummies(df_train['label']).values
y_val   = pd.get_dummies(df_val['label']).values
y_test  = pd.get_dummies(df_test['label']).values

print("y_train shape:", y_train.shape)
print("y_val shape:  ", y_val.shape)
print("y_test shape: ", y_test.shape)

model = Sequential([
    Embedding(num_words, 100, input_length=max_length),
    SpatialDropout1D(0.4),
    LSTM(128, dropout=0.4, recurrent_dropout=0.4),
    Dense(y_train.shape[1], activation='softmax')  # number of classes in y_train
])

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

model.summary()

epochs = 10
batch_size = 32

history = model.fit(
    X_train,
    y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(X_val, y_val),
    verbose=1
)

test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Accuracy: {test_accuracy * 100:.2f}%")

from sklearn.metrics import classification_report
y_test_true = np.argmax(y_test, axis=1)
y_pred = np.argmax(model.predict(X_test), axis=1)
print(classification_report(y_test_true, y_pred))

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test_true, y_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Save the trained model
model.save("sa_text.h5")
print("Model saved to sa_text.h5")

import pickle

# Save the tokenizer to a file (using pickle)
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
print("Tokenizer saved to tokenizer.pickle")

# Save max_length for later use
with open('max_length.pickle', 'wb') as handle:
    pickle.dump(max_length, handle, protocol=pickle.HIGHEST_PROTOCOL)
print("Max length saved to max_length.pickle")

# Define a mapping from numeric labels to emotion names (adjust as needed)
label_mapping = {
    0: "sadness",
    1: "joy",
    2: "love",
    3: "anger",
    4: "fear",
    5: "surprise"
}

# Prompt the user for input text
user_input = input("Enter a text to classify its emotion: ")

# Preprocess the input text with the normalization function
normalized_input = normalize(user_input)

# Convert text to sequence using the trained tokenizer
input_sequence = tokenizer.texts_to_sequences([normalized_input])

# Pad the sequence to match the max_length used during training
input_padded = pad_sequences(input_sequence, maxlen=max_length)

# Use the model to predict the emotion
prediction = model.predict(input_padded)

# Retrieve the predicted class (highest probability)
predicted_numeric_label = np.argmax(prediction, axis=1)[0]

# Map numeric label to the corresponding emotion name
predicted_emotion = label_mapping[predicted_numeric_label]
print("Predicted Emotion:", predicted_emotion)