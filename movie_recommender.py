# -*- coding: utf-8 -*-
"""movie_recommender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ch-4GJGeJqpM1XSoARUk64nDdbA-STpU
"""

import numpy as np
import pandas as pd
import os

df1=pd.read_csv("/content/contentDataGenre.csv")
df2=pd.read_csv("/content/contentDataPrime.csv")
df3=pd.read_csv("/content/contentDataRegion.csv")

df1.head(1)

df_dup1 = df1.duplicated('dataId', keep = False)
df_dup1.value_counts()

df1 = df1.drop_duplicates(subset='dataId')

(df1["genre"].value_counts()).shape # total no of genres

import matplotlib.pyplot as plt
genre_counts = df1["genre"].value_counts()
plt.bar(genre_counts.index, genre_counts.values)
plt.xlabel("Genre")
plt.ylabel("Count")
plt.xticks(rotation=90)
plt.title("Genre Distribution")
plt.show()

df2.head(1)

df_dup1 = df2.duplicated('dataId', keep = False)
df_dup1.value_counts()

df2 = df2.drop_duplicates(subset ='dataId')

df2["gross"].value_counts()

df2["endYear"].value_counts()

df2.drop(["endYear","gross"], axis=1, inplace=True)

df2.info()

df2['certificate'].value_counts()

df2.drop("certificate",axis=1,inplace=True)

df3.head(1)

df_dup1 = df3.duplicated('dataId', keep = False)
df_dup1.value_counts()
df3 = df3.drop_duplicates(subset ='dataId')

merged_df = pd.merge(df3, df1,how='inner', on='dataId')
df = pd.merge(merged_df, df2, how = 'inner', on='dataId')

df.head(1)

df.info()

df['length'] = pd.to_numeric(df['length'], errors='coerce')
df_filtered = df[(df['length'] <= 600) & (df['rating'] >= 8)]

df_filtered

df_filtered1

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from textblob import TextBlob

!pip install nltk
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from textblob import TextBlob

# Download required NLTK data:
nltk.download('stopwords')
nltk.download('punkt')

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Remove non-alphabetic characters
    text = text.lower()  # Convert to lowercase
    text = word_tokenize(text)  # Tokenize text
    text = [word for word in text if word not in stopwords.words('english')]  # Remove stopwords
    return ' '.join(text)

# Apply cleaning function
df_filtered1['clean_plot'] = df_filtered1['description'].dropna().apply(clean_text)

def get_polarity_scores(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity, blob.sentiment.subjectivity

# Apply sentiment analysis
sentiment_scores = df_filtered1['clean_plot'].apply(lambda x: pd.Series(get_polarity_scores(x)))
df_filtered1[['polarity', 'subjectivity']] = sentiment_scores

features = df_filtered1[['length','votes','rating','polarity', 'subjectivity']].dropna()

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
pca = PCA(n_components=2)
reduced_features = pca.fit_transform(features)

# Determine the optimal number of clusters using the Elbow Method
inertia = []
K = range(1, 20)  # Adjust the range based on your data
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=0).fit(features)
    inertia.append(kmeans.inertia_)

plt.plot(K, inertia, 'bx-')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal Clusters')
plt.show()

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3, random_state=0)
df_filtered1['cluster'] = kmeans.fit_predict(features)

from wordcloud import WordCloud
def create_word_cloud(cluster_data, cluster_num):
    text = ' '.join(cluster_data['genre'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'Word Cloud for Cluster {cluster_num}')
    plt.show()

# Create and display word clouds for each cluster
for cluster_num in df_filtered1['cluster'].unique():
    cluster_data = df_filtered1[df_filtered1['cluster'] == cluster_num]
    create_word_cloud(cluster_data, cluster_num)

cluster=df_filtered1.groupby(by=df_filtered1['cluster'])

df_filtered1=cluster.apply(lambda x: x.sort_values(["rating"],ascending=False))
df_filtered1.reset_index(level=0, inplace=True,drop=True)

df_filtered1.head()

import pickle
file = open('dataset.pkl','wb')
pickle.dump(df_filtered1,file)

with open('dataset.pkl', 'rb') as f:
    df_filtered1 = pickle.load(f)
def recommend_movies(cluster_value):
    print('What do you want to see: Movie or TV series?')
    x=input()
    filtered_movies = df_filtered1[df_filtered1['cluster'] == cluster_value]
    if(x=='Movie'):
        top_movies = filtered_movies[filtered_movies['contentType']=='Movie'].head(10)
    else:
        top_movies = filtered_movies[filtered_movies['contentType']=='TV series'].head(10)

    top_movies = filtered_movies.head(10)  # Get the first 10 movies by index
    return top_movies[['dataId', 'title']]

# example usage:0 - Happy,joy,love ; 1- Sad,fear,anger ; 2- neutral,disguist,surprise
recommended_movies = recommend_movies(1)
print(recommended_movies)