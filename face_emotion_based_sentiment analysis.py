# -*- coding: utf-8 -*-
"""Face_Emotion_Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kCL-MMGHFjDfgyxZkv3fvKi2FzRyivGL
"""

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

# Load the dataset
data = pd.read_csv('/fer2013.csv')  # Make sure your CSV path is correct

# Check dataset info
print("Dataset Shape:", data.shape)
print("Emotion labels:", data['emotion'].unique())
print("Usage types:", data["Usage"].unique())

# Prepare images
labels = data['emotion']
pixels = data['pixels']
images = np.zeros((pixels.shape[0], 48 * 48))

for ix in range(images.shape[0]):
    p = pixels[ix].split(' ')
    if len(p) < images.shape[1]:
        p.extend(['0'] * (images.shape[1] - len(p)))
    images[ix] = np.array(p, dtype='float32')

# Visualize few images
plt.figure(figsize=(10, 10))
for index, image in enumerate(images[:9]):
    plt.subplot(3, 3, index + 1)
    plt.imshow(image.reshape(48, 48), cmap='gray')
plt.show()

# Count of images per class
class_counts = data['emotion'].value_counts().sort_index()

# Emotion labels mapping
emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']

# Plot
plt.figure(figsize=(8, 6))
plt.bar(emotion_labels, class_counts, color='skyblue')
plt.title('Number of Images per Emotion Class')
plt.xlabel('Emotion')
plt.ylabel('Number of Images')
plt.grid(axis='y')

# Annotate numbers on bars
for idx, value in enumerate(class_counts):
    plt.text(idx, value + 100, str(value), ha='center', fontsize=10)

plt.show()

# Reshape images to (48, 48, 1)
reshaped_images = images.reshape((-1, 48, 48, 1))

# Normalize pixel values to [0, 1]
norm_images = reshaped_images / 255.0

# One-hot encode the labels
new_labels = to_categorical(labels, num_classes=7)

print("Images shape:", norm_images.shape)
print("Labels shape:", new_labels.shape)

# Split dataset into training and testing
X_train, X_test, y_train, y_test = train_test_split(norm_images, new_labels, test_size=0.2, random_state=42)

print("Train images:", X_train.shape)
print("Test images:", X_test.shape)

# Build CNN model
model = Sequential()

model.add(Conv2D(64, (3,3), padding="same", input_shape=(48,48,1)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(128, (5,5), padding="same"))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(512, (3,3), padding="same"))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(512, (3,3), padding="same"))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Flatten())

model.add(Dense(256))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))

model.add(Dense(512))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))

model.add(Dense(7, activation="softmax"))

# Compile model
opt = Adam(learning_rate=0.0005)
model.compile(optimizer=opt, loss="categorical_crossentropy", metrics=['accuracy'])

# Model summary
model.summary()

# Define callbacks
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-5, verbose=1)
checkpoint = ModelCheckpoint("Fer_Model_new.h5", monitor='val_loss', save_best_only=True, mode='min', verbose=1)

callbacks = [checkpoint, reduce_lr]

# Set epochs (you can increase this later)
epochs = 25

# Start training
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=epochs,
    callbacks=callbacks
)

# Load trained model
from tensorflow import keras
model = keras.models.load_model("Fer_Model_new.h5")

# Import additional libraries
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Define improved preprocessing function
def preprocess_image(image_path):
    img = cv2.imread(image_path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Load Haar Cascade for face detection
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)

    if len(faces) == 0:
        print("No face detected!")
        return None

    # Crop the first detected face
    (x, y, w, h) = faces[0]
    face = gray[y:y+h, x:x+w]

    # Resize to (48, 48)
    face_resized = cv2.resize(face, (48, 48))
    face_resized = face_resized / 255.0  # Normalize

    return face_resized.reshape((1, 48, 48, 1))

print("‚úÖ Model loaded and preprocessing function ready!")

# Define the image path (replace with your actual image path)
image_path = "/sad.jpg"  # üß© Change this to your test image path

# Preprocess the image
preprocessed_image = preprocess_image(image_path)

# Check if preprocessing was successful
if preprocessed_image is not None:
    # Make prediction
    prediction = model.predict(preprocessed_image)
    predicted_class = np.argmax(prediction, axis=-1)[0]

    # Emotion labels
    emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')

    # Print the result
    print(f"Predicted Emotion: {emotions[predicted_class]}")

    # Display the image
    plt.imshow(preprocessed_image.reshape(48, 48), cmap='gray')
    plt.title(f"Predicted: {emotions[predicted_class]}")
    plt.axis('off')
    plt.show()
else:
    print("‚ö†Ô∏è Face not detected in the image. Try another image!")

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode
import cv2
import numpy as np

def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
        async function takePhoto(quality) {
            const div = document.createElement('div');
            const capture = document.createElement('button');
            capture.textContent = 'üì∏ Take Photo';
            div.appendChild(capture);

            const video = document.createElement('video');
            video.style.display = 'block';
            const stream = await navigator.mediaDevices.getUserMedia({video: true});
            document.body.appendChild(div);
            div.appendChild(video);
            video.srcObject = stream;
            await video.play();

            // Resize video to square
            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

            // Wait for capture
            await new Promise((resolve) => capture.onclick = resolve);

            const canvas = document.createElement('canvas');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            canvas.getContext('2d').drawImage(video, 0, 0);
            stream.getTracks().forEach(track => track.stop());
            div.remove();

            return canvas.toDataURL('image/jpeg', quality);
        }
    ''')
    display(js)
    data = eval_js('takePhoto({})'.format(quality))
    binary = b64decode(data.split(',')[1])
    with open(filename, 'wb') as f:
        f.write(binary)
    print(f'üì∑ Photo saved to {filename}')
    return filename

# Capture a photo using the function we defined
captured_image_path = take_photo('/sad.jpg')

# Preprocess the captured image
preprocessed_image = preprocess_image(captured_image_path)

# Check if face was detected
if preprocessed_image is not None:
    # Predict emotion
    prediction = model.predict(preprocessed_image)
    predicted_class = np.argmax(prediction, axis=-1)[0]

    # Emotion labels
    emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')

    # Show prediction
    print(f"Predicted Emotion from Camera: {emotions[predicted_class]}")
    print("predicted_class : ",predicted_class)
    # Show the image
    import matplotlib.pyplot as plt
    plt.imshow(preprocessed_image.reshape(48, 48), cmap='gray')
    plt.title(f"Predicted: {emotions[predicted_class]}")
    plt.axis('off')
    plt.show()
else:
    print("‚ö†Ô∏è No face detected in the captured image.")

# Plot training & validation accuracy values
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid()

plt.show()

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Step 1: Predict on test data
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Step 2: Print classification report
print("Classification Report:\n")
print(classification_report(y_true, y_pred_classes, target_names=('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')))

# Step 3: Confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Step 4: Visualize confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral'),
            yticklabels=('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral'))
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()